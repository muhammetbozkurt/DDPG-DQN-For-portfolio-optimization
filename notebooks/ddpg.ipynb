{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ddpg.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIb1DTXtnfJ5"
      },
      "source": [
        "#Notes\n",
        "\n",
        "* DDPG combines ideas from DPG (Deterministic Policy Gradient) and DQN (Deep Q-Network). It uses Experience Replay and slow-learning target networks from DQN, and it is based on DPG, which can operate over continuous action spaces.\n",
        "\n",
        "* Just like the Actor-Critic method, we have two networks: \n",
        " * Actor - It proposes an action given a state. \n",
        " * Critic - It predicts if the action is good (positive value) or bad (negative value) given a state and an action.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJv75iOgEDJ6"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import keras\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas_datareader.data as web\n",
        "import datetime as dt\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.layers import Lambda, Conv2D, MaxPooling2D, Dropout, Dense, Flatten\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from sklearn.preprocessing import normalize\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mc9ZghfO9-Z9",
        "outputId": "a0ca1793-eb7a-4ae8-9717-cabbcf881dc5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdr57QeCnGU_"
      },
      "source": [
        "# Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2fOYHONnJUw"
      },
      "source": [
        "#Environment params\n",
        "TIMESTEP = 15\n",
        "FEATSNUM = 6\n",
        "TRAIN_TEST = \"2019-05-03\"\n",
        "#eur xau sp nasdaq usd\n",
        "INSTRUMENTNUM = 5\n",
        "INSTRUMENTS = (\"EURUSD=X\", \"GC=F\", \"^GSPC\", \"^IXIC\", \"USD\")\n",
        "\n",
        "\n",
        "EPISODES = 5\n",
        "\n",
        "#model\n",
        "ACTIVATION_ACTOR = \"relu\"\n",
        "ACTIVATION_CRITIC = \"relu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLUfOoVzsx--"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08BjdObusz5d"
      },
      "source": [
        "def normalize(v):\n",
        "    norm = tf.math.reduce_sum(v)\n",
        "    if(norm == 0):\n",
        "        print(v)\n",
        "        print(norm)\n",
        "        result = np.array([[0, 0, 0, 0, 1]])\n",
        "    else:\n",
        "        result = v / norm\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wGDcL1jNphW"
      },
      "source": [
        "class EpisodeData:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.rewards = list()\n",
        "        self.totalOfGoods = list()\n",
        "\n",
        "    def add(self, reward, totalOfGood):\n",
        "        self.rewards.append(reward)\n",
        "        self.totalOfGoods.append(totalOfGood)\n",
        "\n",
        "    def plot(self):\n",
        "        plt.plot(self.totalOfGoods)\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-jfRvWiMAH2"
      },
      "source": [
        "# For Noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgMPypTfL_ty"
      },
      "source": [
        "class OUActionNoise:\n",
        "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
        "        self.theta = theta\n",
        "        self.mean = mean\n",
        "        self.std_dev = std_deviation\n",
        "        self.dt = dt\n",
        "        self.x_initial = x_initial\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
        "        x = (\n",
        "            self.x_prev\n",
        "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
        "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
        "        )\n",
        "        # Store x into x_prev\n",
        "        # Makes next noise dependent on current one\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        if self.x_initial is not None:\n",
        "            self.x_prev = self.x_initial\n",
        "        else:\n",
        "            self.x_prev = np.zeros_like(self.mean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk1SG3Axqxpt"
      },
      "source": [
        "# Environment Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Lo0OWYv4zBl"
      },
      "source": [
        "\n",
        "\n",
        "*   Alım yaparken bir önceki is gününün kapanış fiyatına göre yapıyor\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0JtaSrFJNw8"
      },
      "source": [
        "class Market:\n",
        "    def __init__(self, totalOfGoods : float = 1000., howToReorganize : bool = False ,startDateOfMarket : dt.datetime =  dt.datetime(2004,1,1), endDateOfMarket : dt.datetime =  dt.datetime(2018,1,1)):\n",
        "        #hold copy of totalOfGoods for reseting environment\n",
        "        self._startGoodsValue = totalOfGoods\n",
        "        self.bigData = list()\n",
        "        self._initializationOfVariables()\n",
        "        self._lastDate = 0\n",
        "        self.howToReorganize = howToReorganize\n",
        "\n",
        "        self.startDateOfMarket = startDateOfMarket\n",
        "        self.endDateOfMarket = endDateOfMarket\n",
        "        \n",
        "\n",
        "    def _retrieveData(self) -> None:\n",
        "        \"\"\"\n",
        "        Retrieve instuments data from market\n",
        "\n",
        "        Returns None\n",
        "        \"\"\"\n",
        "        for instrument in INSTRUMENTS:\n",
        "            if (instrument == \"USD\"):\n",
        "                continue\n",
        "            df = web.DataReader(instrument, \"yahoo\", self.startDateOfMarket, self.endDateOfMarket)\n",
        "            self.bigData.append(df)\n",
        "\n",
        "        if (self.howToReorganize):\n",
        "            self.reorganizeBigDataGetAll()\n",
        "        else:\n",
        "            self.reorganizeBigData()\n",
        "    \n",
        "    def reorganizeBigData(self):\n",
        "        \"\"\"\n",
        "        burada hepsini euro dolara göre aldım\n",
        "        \"\"\"\n",
        "\n",
        "        self.bigData[0] = self.bigData[0].shift(1, freq = \"D\")\n",
        "        usaIndex = set(self.bigData[0].index)\n",
        "        goldIndex = set(self.bigData[1].index)\n",
        "        londonIndex = set(self.bigData[1].index)\n",
        "\n",
        "        reIndex = usaIndex.union(goldIndex)\n",
        "        self.bigData[1] = self.bigData[1].reindex(list(reIndex)).sort_index().ffill().bfill().reindex(self.bigData[0].index).sort_index().ffill().bfill()\n",
        "\n",
        "        reIndex = usaIndex.union(londonIndex)\n",
        "        self.bigData[2] = self.bigData[2].reindex(list(reIndex)).sort_index().ffill().bfill().reindex(self.bigData[0].index).sort_index().ffill().bfill()\n",
        "\n",
        "        self.bigData[3] = self.bigData[3].reindex(list(reIndex)).sort_index().ffill().bfill().reindex(self.bigData[0].index).sort_index().ffill().bfill()\n",
        "\n",
        "    def reorganizeBigDataGetAll(self):\n",
        "        \"\"\"\n",
        "        nu metodla hepsinin günlerini esitledim\n",
        "        \"\"\"\n",
        "        \n",
        "        reIndex = set ()\n",
        "        for df in self.bigData:\n",
        "            temp = set (df.index)\n",
        "            reIndex = reIndex.union(temp)\n",
        "        \n",
        "        for i in range(len(self.bigData)):\n",
        "            self.bigData[i] = self.bigData[i].reindex(reIndex).ffill()\n",
        "            self.bigData[i].bfill(inplace = True)\n",
        "            self.bigData[i].sort_index(inplace=True)\n",
        "\n",
        "    def _initializationOfVariables(self) -> None:\n",
        "        \"\"\"\n",
        "        Every time user call reset method _initializationOfVariables method invoked automatically to reset necessary variables\n",
        "        to their initial values\n",
        "\n",
        "        Returns None\n",
        "        \"\"\"\n",
        "        \n",
        "        self.totalOfGoods = self._startGoodsValue\n",
        "        self.analizeStartDate = 0 \n",
        "        self.timeInterval = TIMESTEP\n",
        "        self.currentDate = TIMESTEP\n",
        "\n",
        "        # basket is a list that holds how much of instruments is in our basket by its unit \n",
        "        self.basket = np.zeros(INSTRUMENTNUM)\n",
        "        self.weights = np.zeros(INSTRUMENTNUM)\n",
        "\n",
        "        self.weights[INSTRUMENTNUM-1] = 1\n",
        "        self.basket[INSTRUMENTNUM-1] = self.totalOfGoods\n",
        "        \n",
        "    \n",
        "    def start(self) -> np.array:\n",
        "        \"\"\"\n",
        "        Start environment by retreiving necessary market data.\n",
        "\n",
        "        Returns state\n",
        "        \"\"\"\n",
        "        self._retrieveData()\n",
        "        #set last date\n",
        "        self._lastDate = len(self.bigData[0])\n",
        "        \n",
        "        state = self._getCurrentState()\n",
        "        return state\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self._initializationOfVariables()\n",
        "        state =  self._getCurrentState()\n",
        "        return state\n",
        "\n",
        "\n",
        "\n",
        "    def _getCurrentState(self) -> np.array:\n",
        "        \"\"\"\n",
        "        Calculate current state respective to self.currentDate\n",
        "\n",
        "        Returns current State of market as a np.array\n",
        "        \"\"\"\n",
        "        state = list ()\n",
        "        for instrumentDF in self.bigData:\n",
        "            df = instrumentDF.iloc[self.analizeStartDate:self.currentDate,:].copy()\n",
        "            \n",
        "            #normalize data and rearrange headers\n",
        "            self.preprocessRawData(df)\n",
        "            state.append(df)\n",
        "        \n",
        "        #its Shape is (INSTRUMENTNUM - 1,TIMESTEP,FEATSNUM,1) \n",
        "        return np.array(state).reshape((-1,TIMESTEP,FEATSNUM,1))\n",
        "\n",
        "    def preprocessRawData(self, df) -> None:\n",
        "        # High       Low      Open     Close  Volume  Adj Close\n",
        "        df[\"H\"] = df.loc[:, \"High\"] / df.iloc[0, 0]\n",
        "        df[\"L\"] = df.loc[:, \"Low\"] / df.iloc[0, 1]\n",
        "        df[\"H-Step\"] = (df.loc[:, \"High\"] - df.loc[:, \"Low\"]) / df.loc[:, \"Low\"]\n",
        "        #df[\"C\"] = df.loc[:, \"Close\"] / df.iloc[0, \"Close\"] \n",
        "        df[\"O\"] = df.loc[:, \"Open\"] / df.iloc[0, 2]\n",
        "        df[\"C-Step\"] = (df.loc[:, \"Close\"] - df.loc[:, \"Open\"] ) / df.loc[:, \"Open\"]\n",
        "        df.loc[:, \"Adj Close\"] = df.loc[:, \"Adj Close\"] / df.iloc[0, 5]\n",
        "        df.drop([\"Volume\", \"High\", \"Close\", \"Low\", \"Open\"],axis=1 , inplace= True)\n",
        "\n",
        "\n",
        "    def _buyAllInstruments(self, logFile):\n",
        "        \n",
        "        loss : float = 0\n",
        "\n",
        "        closePrices = list ()\n",
        "\n",
        "        for index in range(INSTRUMENTNUM-1):\n",
        "            closePrice = self.bigData[index].iloc[self.currentDate-1 , 3]\n",
        "            \n",
        "            \n",
        "            closePrices.append(closePrice)\n",
        "            \n",
        "            \n",
        "            unitLoss = self.totalOfGoods * self.weights[index]\n",
        "            loss += unitLoss\n",
        "            self.basket[index] =  unitLoss / closePrice if ( unitLoss != 0 ) else 0\n",
        "        \n",
        "        #for dolar\n",
        "        index += 1\n",
        "        self.basket[index] = self.totalOfGoods * self.weights[index]\n",
        "\n",
        "        logFile.write(f\"buy prices: {closePrices}\\n\")\n",
        "        logFile.write(f\"basket after buy: {self.basket}\\n\")\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def _sellAllInstruments(self, logFile) -> float:\n",
        "        \"\"\"\n",
        "        this method sells all instruments and adds profit to \"totalOfGoods\"\n",
        "        \"\"\"\n",
        "        profit = 0\n",
        "\n",
        "\n",
        "        closePrices = list ()\n",
        "\n",
        "        #because last one is usd\n",
        "        for index in range(INSTRUMENTNUM-1):\n",
        "            closePrice = self.bigData[index].iloc[self.currentDate, 3]\n",
        "\n",
        "\n",
        "            closePrices.append(closePrice)\n",
        "\n",
        "\n",
        "            unitProfit = closePrice * self.basket[index]\n",
        "            profit += unitProfit\n",
        "            self.basket[index] = 0. \n",
        "        \n",
        "\n",
        "        logFile.write(f\"sell prices: {closePrices}\\n\")\n",
        "        \n",
        "\n",
        "        index += 1\n",
        "        self.totalOfGoods = profit + self.basket[index] if (profit + self.basket[index] != 0) else self.totalOfGoods\n",
        "        return profit\n",
        "\n",
        "    \n",
        "    def incrementDay(self):\n",
        "        \"\"\"\n",
        "        This method increment currentDate and _startDate and returns a bool value\n",
        "        if new currentDate value is either more than or equal to _last day in our data then it returns False\n",
        "        else it returns True\n",
        "        \"\"\"\n",
        "        self.currentDate += 1\n",
        "        self.analizeStartDate =  int(self.currentDate - self.timeInterval)\n",
        "\n",
        "        done = self.currentDate >= self._lastDate - 1\n",
        "        return done\n",
        "\n",
        "\n",
        "    def step(self, weights : np.array, logFile, normalizedReward : float = False):\n",
        "        \"\"\"\n",
        "        Buy instruments at last days close\n",
        "        sell them all at close\n",
        "        increment day\n",
        "\n",
        "        weights: is an np.array that holds how to disturbue total money to insturuments which is also action\n",
        "        normalizedReward: if it is true than returns normalized profit else normal profit default False \n",
        "\n",
        "        Returns (nextState, reward, done)\n",
        "        \"\"\"\n",
        "        sumOfWeights = tf.math.reduce_sum(weights)\n",
        "        #if(0.9< sumOfWeights <1.1):\n",
        "        #    self.weights = np.array(weights).reshape((5,))\n",
        "        #else:\n",
        "        #    print(f\"Weights has problems their sum is not one\\nsum: {sumOfWeights}\")\n",
        "        #    print(f\"State {self._getCurrentState()}\")\n",
        "        #    return False\n",
        "        self.weights = np.array(weights)\n",
        "        oldTotalOfGoods = self.totalOfGoods\n",
        "        loss = self._buyAllInstruments(logFile)\n",
        "        self.basketCopy = self.basket.copy()\n",
        "        profit = self._sellAllInstruments(logFile)\n",
        "\n",
        "        \n",
        "        reward = (profit - loss) / oldTotalOfGoods if normalizedReward else profit - loss\n",
        "        \n",
        "        logFile.write(f\"current day: {self.currentDate}\\n\")\n",
        "        logFile.write(f\"last day: {self._lastDate}\\n\")\n",
        "        logFile.write(f\"sum of weights: {sumOfWeights}\\n\")\n",
        "        logFile.write(f\"weights: {self.weights}\\n\")\n",
        "        logFile.write(f\"profit: {profit}\\n\")\n",
        "        logFile.write(f\"loss: {loss}\\n\")\n",
        "        logFile.write(f\"profit-loss: {profit - loss}\\n\")\n",
        "        logFile.write(f\"reward: {reward}\\n\")\n",
        "        logFile.write(f\"old totalOfGoods: {oldTotalOfGoods}\\n\")\n",
        "        logFile.write(f\"new totalOfGoods: {self.totalOfGoods}\\n\\n\")\n",
        "        done = self.incrementDay()\n",
        "\n",
        "        nextState = self._getCurrentState()\n",
        "\n",
        "        return (nextState, reward, done)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMl8MOHOWyPi"
      },
      "source": [
        "# DDPG kısmı komple degisecek\n",
        "\n",
        "*   https://keras.io/api/models/model/\n",
        "*   https://keras.io/examples/rl/ddpg_pendulum/\n",
        "*   https://www.youtube.com/watch?v=4jh32CvwKYw\n",
        "\n",
        "\n",
        "learning rate of critic network is sileltly higher than actor network and that is because in general in policy gradiend based methods the polict aproximation is a little bit more sensitive to the perturbation of and parameters\n",
        "\n",
        "### GradientTape\n",
        "\n",
        "*   https://medium.com/analytics-vidhya/tf-gradienttape-explained-for-keras-users-cc3f06276f22\n",
        "\n",
        "If at any point, we want to use multiple variables in our calculations, all we need to do is give tape.gradient a list or tuple of those variables. When we optimize Keras models, we pass model.trainable_variables as our variable list.\n",
        "\n",
        "\n",
        "This is because immediately after calling tape.gradient, the GradientTape releases all the information stored inside of it for computational purposes.\n",
        "If we want to bypass this, we can set persistent=True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utU1Im4DXs8v"
      },
      "source": [
        "# Replay Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVHopPxvXvzi"
      },
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, maxSize, inputShape, nActions):\n",
        "        self.memberSize = maxSize\n",
        "        self.memberCounter = 0\n",
        "        self.stateMemory = np.zeros((self.memberSize, *inputShape))\n",
        "        self.newStateMemory = np.zeros((self.memberSize, *inputShape))\n",
        "        self.actionMemory = np.zeros((self.memberSize, nActions))\n",
        "        self.rewardMemory = np.zeros((self.memberSize))\n",
        "        #it indicates that self.memberCounter exceeds self.memberSize at least once\n",
        "        self.sizeFlag = False\n",
        "\n",
        "        #terminal state i hafızada tutmak istemiyorum\n",
        "\n",
        "    def storeTransition(self, state : np.array, action : np.array, reward : float, newState : np.array) -> None:\n",
        "\n",
        "        self.stateMemory[self.memberCounter] = state\n",
        "        self.actionMemory[self.memberCounter] = action\n",
        "        self.rewardMemory[self.memberCounter] = reward\n",
        "        self.newStateMemory[self.memberCounter] = newState\n",
        "\n",
        "        self.memberCounter += 1\n",
        "        \n",
        "        if(self.memberCounter == self.memberSize):\n",
        "            self.sizeFlag = True \n",
        "\n",
        "        self.memberCounter = self.memberCounter % self.memberSize\n",
        "        \n",
        "\n",
        "    \n",
        "    def sampleBuffer(self, batchSize : int) -> np.array:\n",
        "        maxIndex = self.memberSize if self.sizeFlag else self.memberCounter\n",
        "\n",
        "        batch = np.random.choice(maxIndex, batchSize, replace = False)\n",
        "\n",
        "        states = self.stateMemory[batch]\n",
        "        newStates = self.newStateMemory[batch]\n",
        "        actions = self.actionMemory[batch]\n",
        "        rewards = self.rewardMemory[batch]\n",
        "\n",
        "        return states, actions, rewards, newStates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eSYoZywYBt-"
      },
      "source": [
        "# ActorNetwork"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy49KpMvYI2O"
      },
      "source": [
        "class ActorNetwork(keras.Model):\n",
        "    def __init__(self, nActions = 5, modelName : str = \"Actor\", checkPointDirectory : str = \"saves/ddpg\"):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "\n",
        "        self.modelName = modelName\n",
        "        self.checkpointDir = checkPointDirectory \n",
        "        self.checkpointFile = os.path.join(self.checkpointDir, \n",
        "                    self.modelName+'_ddpg.h5')\n",
        "\n",
        "        self.inputLayer = Dense(1024, dtype = \"float32\", activation = ACTIVATION_ACTOR)\n",
        "        self.inputLayer_ = Dense(1024, dtype = \"float32\", activation = ACTIVATION_ACTOR)\n",
        "        self.interLayer3 = Dense(1024, dtype = \"float32\", activation = ACTIVATION_ACTOR)\n",
        "        self.interLayer4 = Dense(512, dtype = \"float32\", activation = ACTIVATION_ACTOR)\n",
        "        self.mu = Dense(nActions, dtype = \"float32\", activation= \"softmax\")\n",
        "\n",
        "    def call(self, state):\n",
        "        #state = tf.convert_to_tensor([state], dtype = tf.float32)\n",
        "        prob = self.inputLayer(state)\n",
        "        prob = self.inputLayer_(prob)\n",
        "        prob = self.interLayer3(prob)\n",
        "        prob = self.interLayer4(prob)\n",
        "\n",
        "        mu = self.mu(prob)\n",
        "        #total of them must be 1\n",
        "        #mu = normalize(mu)\n",
        "\n",
        "        return mu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucIlGbcMYFnr"
      },
      "source": [
        "# ActorNetwork"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfLMqB-oX-9h"
      },
      "source": [
        "class CriticNetwork(keras.Model):\n",
        "    def __init__(self, modelName : str = \"critic\", checkPointDirectory : str = \"saves/ddpg\") -> None:\n",
        "        super(CriticNetwork, self).__init__()\n",
        "\n",
        "        self.modelName = modelName\n",
        "        self.checkpointDir = checkPointDirectory \n",
        "        self.checkpointFile = os.path.join(self.checkpointDir, \n",
        "                    self.modelName+'_ddpg.h5')\n",
        "\n",
        "        self.inputLayer = Dense(512, activation = ACTIVATION_CRITIC, name = \"input yeri\")\n",
        "        self.interLayer1 = Dense(1024, activation = ACTIVATION_CRITIC)\n",
        "        self.interLayer2 = Dense(1024, activation = ACTIVATION_CRITIC)\n",
        "        self.interLayer3 = Dense(512, activation = ACTIVATION_CRITIC)\n",
        "        self.q = Dense(1, name=\"cikti\")\n",
        "\n",
        "    \n",
        "    def call(self, state, action):\n",
        "        actionValue = self.inputLayer(tf.concat([state, action], axis=1))\n",
        "        actionValue = self.interLayer1(actionValue)\n",
        "        actionValue = self.interLayer2(actionValue)\n",
        "        actionValue = self.interLayer3(actionValue)\n",
        "\n",
        "        qValue = self.q(actionValue)\n",
        "\n",
        "        return qValue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrMYaHTmm8Gh"
      },
      "source": [
        "# DDPG Agent Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23BEfDTtZ4ID"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, inputDims, alpha = 0.01, beta = 0.02, actionMax = 1., \\\n",
        "        actionMin : float = 0, gamma = 0.99, nActions = 5, maxSize = 100000, tau = 0.005,\\\n",
        "        batchSize = 64, noise = 0.1):\n",
        "        \"\"\"\n",
        "        alpha is learning rate for actor network\n",
        "        beta is learning rate for critic network\n",
        "        actionMax is max limit of action that agent can take \n",
        "        actionMin is min limit of action that agent can take \n",
        "        gama is discount foctor for update equation\n",
        "        maxSize is limit for replay buffer\n",
        "        tau is soft update factor\n",
        "        noise is stddev valu of noise that is going to use for exploration\n",
        "\n",
        "        beta is slightly higher than alpa and that is because in general in policy gradient type methods the policy approximation is a little bit more sensitive to\n",
        "        pertubation and parameters.\n",
        "\n",
        "        actionMax and actionMin are going to use for clipping after noise added to action for exploration\n",
        "        \n",
        "        default value of tau came from paper\n",
        "        \"\"\"\n",
        "\n",
        "        self.gamma = gamma \n",
        "        self.tau = tau\n",
        "        self._memory = ReplayBuffer(maxSize, inputDims, nActions)\n",
        "        self.batchSize = batchSize\n",
        "        self.nActions = nActions\n",
        "        self.noise = noise\n",
        "        self.maxAction = actionMax\n",
        "        self.minAction = actionMin\n",
        "\n",
        "        self.actor = ActorNetwork(nActions=nActions)\n",
        "        self.critic = CriticNetwork()\n",
        "\n",
        "        self.targetActor = ActorNetwork(nActions=nActions)\n",
        "        self.targetCritic = CriticNetwork()\n",
        "\n",
        "        self.actor.compile(optimizer= Adam(alpha))\n",
        "        self.critic.compile(optimizer=Adam(beta))\n",
        "\n",
        "        self.targetActor.compile(optimizer= Adam(alpha))\n",
        "        self.targetCritic.compile(optimizer=Adam(beta))\n",
        "        \n",
        "        #tau is 1 because ve want hard copy\n",
        "        self.updateNetworkParameters(tau = 1)\n",
        "\n",
        "\n",
        "    def updateNetworkParameters(self, tau =None):\n",
        "        \n",
        "        tau = self.tau if tau is None else tau\n",
        "\n",
        "        #soft update to target actor network\n",
        "        networkWeights = []\n",
        "        targetWeights = self.targetActor.weights\n",
        "        for i, layerWeights in enumerate(self.actor.weights):\n",
        "            networkWeights.append(layerWeights * tau + targetWeights[i] * (1 - tau))\n",
        "        self.targetActor.set_weights(networkWeights)\n",
        "\n",
        "        #soft update to target critic network\n",
        "        networkWeights = []\n",
        "        targetWeights = self.targetCritic.weights\n",
        "        for i, layerWeights in enumerate(self.critic.weights):\n",
        "            networkWeights.append(layerWeights * tau + targetWeights[i] * (1 - tau))\n",
        "        self.targetCritic.set_weights(networkWeights) \n",
        "\n",
        "\n",
        "    def storeMemory(self, state, action , reward, newState):\n",
        "        self._memory.storeTransition(state, action, reward, newState)\n",
        "\n",
        "    def chooseAction(self, state, evaluate = False):\n",
        "        state = tf.convert_to_tensor([state], dtype = tf.float32)\n",
        "        actions = self.actor(state)\n",
        "        if not evaluate:\n",
        "            actions += tf.random.normal(shape=[self.nActions], \\\n",
        "                mean=0.0, stddev = self.noise, dtype = tf.float32)\n",
        "            actions = tf.clip_by_value(actions, self.minAction, self.maxAction)\n",
        "            actions = normalize(actions)\n",
        "        \n",
        "        return actions[0]\n",
        "\n",
        "    def learn(self):\n",
        "        if self._memory.memberCounter < self.batchSize:\n",
        "            return\n",
        "        states, actions, reward_, newStates = self._memory.sampleBuffer(self.batchSize)\n",
        "\n",
        "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
        "        newStates = tf.convert_to_tensor(newStates, dtype=tf.float32)\n",
        "        rewards = tf.convert_to_tensor(reward_, dtype=tf.float32)\n",
        "        actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
        "\n",
        "        #q = reward + gamma * q_next\n",
        "        #purpose of critic network is find q value of action for corresponding state\n",
        "        #therefore, loss is difference of q values\n",
        "        \n",
        "        #with tf.GradientTape as tape:\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            targetActions = self.targetActor(newStates)\n",
        "            newCriticValue = tf.squeeze(self.targetCritic(newStates, targetActions), 1)\n",
        "\n",
        "            criticValue = tf.squeeze(self.critic(states, actions), 1)\n",
        "            \n",
        "            target = reward + self.gamma * newCriticValue\n",
        "            criticLoss = keras.losses.MSE(target, criticValue)\n",
        "        #tape.__exit__()\n",
        "\n",
        "        #find gradient\n",
        "        criticNetworkGradient = tape.gradient(criticLoss, self.critic.non_trainable_variables)\n",
        "\n",
        "        #optimize weights\n",
        "        self.critic.optimizer.apply_gradients(zip(\n",
        "            criticNetworkGradient, self.critic.trainable_variables\n",
        "        ))\n",
        "\n",
        "        #actor try to maximize q value \n",
        "        #Hence loss is -q\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            newPolicyActions = self.actor(states)\n",
        "            actorLoss = -self.critic(states, newPolicyActions)\n",
        "            actorLoss = tf.math.reduce_mean(actorLoss)\n",
        "        #tape.__exit__()\n",
        "\n",
        "        actorNetworkGradient = tape.gradient(actorLoss,\\\n",
        "            self.actor.trainable_variables)\n",
        "        \n",
        "        self.actor.optimizer.apply_gradients(zip(\n",
        "            actorNetworkGradient, self.actor.trainable_variables\n",
        "        ))\n",
        "\n",
        "        self.updateNetworkParameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5njHKa0Gw9C-",
        "outputId": "ce5b8b01-a663-4a07-b43c-d8a3f2c72101"
      },
      "source": [
        "inputDims = TIMESTEP * FEATSNUM * (INSTRUMENTNUM - 1)\n",
        "agent = Agent([inputDims], noise = 0.2, gamma=0.9, tau=0.05)\n",
        "env = Market(startDateOfMarket=dt.datetime(2018,1,1), endDateOfMarket=dt.datetime(2018,4,1))\n",
        "startState = env.start()\n",
        "\n",
        "\n",
        "#for testing\n",
        "testEnv = Market(startDateOfMarket=dt.datetime(2018,1,1), endDateOfMarket=dt.datetime(2018,4,1))\n",
        "groundTruhth = Market(startDateOfMarket=dt.datetime(2018,1,1), endDateOfMarket=dt.datetime(2018,4,1))\n",
        "\n",
        "testEnv.start()\n",
        "groundTruhth.start()\n",
        "print(\"\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbY-Z7IGZygY"
      },
      "source": [
        "# Test\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Iw87Or71Ggy"
      },
      "source": [
        "def test(agent : Agent):\n",
        "    current_state = testEnv.reset().flatten()\n",
        "    groundTruhth.reset()\n",
        "    episodeLog = EpisodeData()\n",
        "    episodeLogGT = EpisodeData()\n",
        "\n",
        "    savePath = \"/content/drive/MyDrive/DDPG/test/\"\n",
        "    done = False\n",
        "    step = 0\n",
        "    action = np.array([0] * 5)\n",
        "\n",
        "    if(not os.path.exists(f\"{savePath}\")):\n",
        "        os.mkdir(f\"{savePath}\")\n",
        "        os.mkdir(f\"{savePath}plots/\")\n",
        "\n",
        "    Log_file_name = f\"testLog_{dt.datetime.now()}\"\n",
        "    Log_file_name_gt = f\"{savePath}groundTruthLog_{dt.datetime.now()}.txt\"\n",
        "    logFile = open(savePath+Log_file_name+\".txt\", \"w\")\n",
        "    groundTruthLogFile = open(f\"{savePath}groundTruthLog_{dt.datetime.now()}.txt\", \"w\")\n",
        "    \n",
        "    while (not done):\n",
        "        logFile.write(\"-\"*5 + f\"Turn: {step} \"+ \"-\"*5 + \"\\n\")\n",
        "        groundTruthLogFile.write(\"-\"*5 + f\"Turn: {step} \"+ \"-\"*5 + \"\\n\")\n",
        "\n",
        "        _, reward_, _ =  groundTruhth.step(np.array([0.2] * 5), groundTruthLogFile, normalizedReward=False)\n",
        "\n",
        "        episodeLogGT.add(reward_, groundTruhth.totalOfGoods)\n",
        "\n",
        "\n",
        "        oldAction = action\n",
        "        action = agent.chooseAction(current_state, evaluate = True)\n",
        "        print(\"action: \",np.mean(action - oldAction))\n",
        "        next_state, reward, done =  testEnv.step(action, logFile, normalizedReward=False)\n",
        "        print(\"state: \", np.mean(current_state - next_state.flatten()))\n",
        "        episodeLog.add(reward, testEnv.totalOfGoods)\n",
        "\n",
        "        current_state = next_state.flatten()\n",
        "        step += 1\n",
        "    \n",
        "    logFile.close()\n",
        "    groundTruthLogFile.close()\n",
        "    \n",
        "    plt.plot(episodeLog.totalOfGoods, label=\"agent\")\n",
        "    plt.plot(episodeLogGT.totalOfGoods, label=\"GT\")\n",
        "    plt.legend()\n",
        "    plt.savefig(savePath + \"plots/\" +Log_file_name + \".png\")\n",
        "    plt.clf()\n",
        "    return episodeLog, episodeLogGT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48frP_Jk-O7H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "02b429ba-8cd3-4188-9463-b78523844f77"
      },
      "source": [
        "loadCheckPoint = False\n",
        "\n",
        "episodeLogs = list ()\n",
        "\n",
        "saveFile = \"/content/drive/MyDrive/DDPG/train/\"\n",
        "\n",
        "for i in range(EPISODES):\n",
        "    state = env.reset().flatten()\n",
        "    done = False\n",
        "    j = 0\n",
        "    \n",
        "    logFile = open(f\"{saveFile}Episode_{i}.txt\", \"w\")\n",
        "    episodeLog = EpisodeData()\n",
        "    episodeLogs.append(episodeLog)\n",
        "    while not done:\n",
        "        logFile.write(\"-\"*5 + f\" Episode: {i}, Turn: {j} \"+ \"-\"*5 + \"\\n\")\n",
        "        action = agent.chooseAction(state, False)\n",
        "        actionFloat64 = np.array(normalize(action)).astype(dtype=\"float64\")\n",
        "        stateNew, reward, done = env.step(action, logFile, normalizedReward=False)\n",
        "        stateNew = stateNew.flatten()\n",
        "        agent.storeMemory(state, action, reward, stateNew)\n",
        "        episodeLog.add(reward, env.totalOfGoods)\n",
        "\n",
        "        if not loadCheckPoint:\n",
        "            agent.learn()\n",
        "        state = stateNew\n",
        "        j += 1\n",
        "    agent.actor.save_weights(f\"{saveFile}actor_{i}.h5\", overwrite=True)\n",
        "    agent.critic.save_weights(f\"{saveFile}critic_{i}.h5\", overwrite=True)\n",
        "    logFile.close()\n",
        "    test(agent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "action:  0.19999999\n",
            "state:  0.002456759646384908\n",
            "action:  1.4901161e-09\n",
            "state:  0.001094414514188486\n",
            "action:  5.9604646e-09\n",
            "state:  -0.0002076058957797862\n",
            "action:  -8.940697e-09\n",
            "state:  -0.0013107690322087136\n",
            "action:  2.9802323e-09\n",
            "state:  -0.001340120847055578\n",
            "action:  4.4703485e-09\n",
            "state:  -0.0023473197142025496\n",
            "action:  2.3841858e-08\n",
            "state:  0.0006913633320925117\n",
            "action:  -1.0430813e-08\n",
            "state:  0.003018140809961992\n",
            "action:  -7.450581e-09\n",
            "state:  0.0008206505309369905\n",
            "action:  -5.9604646e-09\n",
            "state:  0.003051325342322472\n",
            "action:  -8.940697e-09\n",
            "state:  0.0015101726036576353\n",
            "action:  2.3841858e-08\n",
            "state:  0.0008928932458982505\n",
            "action:  -5.9604646e-09\n",
            "state:  0.001726101304245578\n",
            "action:  -4.4703485e-09\n",
            "state:  0.002070854221233108\n",
            "action:  -1.3411045e-08\n",
            "state:  0.0066056821916167085\n",
            "action:  2.2351742e-08\n",
            "state:  0.004153103372200217\n",
            "action:  -4.4703485e-09\n",
            "state:  0.0018413250989357422\n",
            "action:  0.0\n",
            "state:  0.0033698642764713755\n",
            "action:  -1.4901161e-09\n",
            "state:  0.000641422404619827\n",
            "action:  -4.4703485e-09\n",
            "state:  -0.004341935857558891\n",
            "action:  1.4901161e-09\n",
            "state:  0.0008748338168690172\n",
            "action:  2.9802323e-09\n",
            "state:  0.0009000304647625658\n",
            "action:  8.940697e-09\n",
            "state:  -0.003628900400384709\n",
            "action:  -1.9371509e-08\n",
            "state:  0.0011266494539786348\n",
            "action:  7.450581e-09\n",
            "state:  -0.016989338878244564\n",
            "action:  -2.9802323e-09\n",
            "state:  0.002607762884756028\n",
            "action:  2.9802323e-09\n",
            "state:  -0.009138651606971138\n",
            "action:  1.4901161e-08\n",
            "state:  -0.006578079549426104\n",
            "action:  -2.9802322e-08\n",
            "state:  -0.0008832843991276614\n",
            "action:  2.3841858e-08\n",
            "state:  0.00800443314789029\n",
            "action:  -1.4901161e-08\n",
            "state:  0.005140939744550815\n",
            "action:  1.1920929e-08\n",
            "state:  0.007525673687201473\n",
            "action:  1.4901161e-09\n",
            "state:  0.0037310422071848304\n",
            "action:  -2.0861625e-08\n",
            "state:  5.970137193783235e-05\n",
            "action:  7.450581e-09\n",
            "state:  -0.004310135628423221\n",
            "action:  7.450581e-09\n",
            "state:  -0.001871409335650305\n",
            "action:  1.4901161e-08\n",
            "state:  -0.002335883628259506\n",
            "action:  -2.9802323e-09\n",
            "state:  0.001936613709020266\n",
            "action:  -7.450581e-09\n",
            "state:  0.0001267137384604404\n",
            "action:  -4.4703485e-09\n",
            "state:  0.0029498825992987157\n",
            "action:  1.4901161e-08\n",
            "state:  -0.0034497089092379407\n",
            "action:  -1.3411045e-08\n",
            "state:  -0.00720330722626482\n",
            "action:  -8.940697e-09\n",
            "state:  -0.0016003848613129984\n",
            "action:  1.4901161e-08\n",
            "state:  0.001425090617915552\n",
            "action:  -1.3411045e-08\n",
            "state:  0.009412589363341488\n",
            "action:  -7.450581e-09\n",
            "state:  0.0008286072265496772\n",
            "action:  1.0430813e-08\n",
            "state:  0.0026387626667763618\n",
            "action:  1.6391278e-08\n",
            "state:  0.004519316687791081\n",
            "action:  -2.9802322e-08\n",
            "state:  9.73893621877425e-05\n",
            "action:  1.3411045e-08\n",
            "state:  0.0038689734131781483\n",
            "action:  0.2\n",
            "state:  0.002456759646384908\n",
            "action:  0.0\n",
            "state:  0.001094414514188486\n",
            "action:  0.0\n",
            "state:  -0.0002076058957797862\n",
            "action:  0.0\n",
            "state:  -0.0013107690322087136\n",
            "action:  0.0\n",
            "state:  -0.001340120847055578\n",
            "action:  0.0\n",
            "state:  -0.0023473197142025496\n",
            "action:  0.0\n",
            "state:  0.0006913633320925117\n",
            "action:  0.0\n",
            "state:  0.003018140809961992\n",
            "action:  0.0\n",
            "state:  0.0008206505309369905\n",
            "action:  0.0\n",
            "state:  0.003051325342322472\n",
            "action:  0.0\n",
            "state:  0.0015101726036576353\n",
            "action:  0.0\n",
            "state:  0.0008928932458982505\n",
            "action:  0.0\n",
            "state:  0.001726101304245578\n",
            "action:  0.0\n",
            "state:  0.002070854221233108\n",
            "action:  0.0\n",
            "state:  0.0066056821916167085\n",
            "action:  0.0\n",
            "state:  0.004153103372200217\n",
            "action:  0.0\n",
            "state:  0.0018413250989357422\n",
            "action:  0.0\n",
            "state:  0.0033698642764713755\n",
            "action:  0.0\n",
            "state:  0.000641422404619827\n",
            "action:  0.0\n",
            "state:  -0.004341935857558891\n",
            "action:  0.0\n",
            "state:  0.0008748338168690172\n",
            "action:  0.0\n",
            "state:  0.0009000304647625658\n",
            "action:  0.0\n",
            "state:  -0.003628900400384709\n",
            "action:  0.0\n",
            "state:  0.0011266494539786348\n",
            "action:  0.0\n",
            "state:  -0.016989338878244564\n",
            "action:  0.0\n",
            "state:  0.002607762884756028\n",
            "action:  0.0\n",
            "state:  -0.009138651606971138\n",
            "action:  0.0\n",
            "state:  -0.006578079549426104\n",
            "action:  0.0\n",
            "state:  -0.0008832843991276614\n",
            "action:  0.0\n",
            "state:  0.00800443314789029\n",
            "action:  0.0\n",
            "state:  0.005140939744550815\n",
            "action:  0.0\n",
            "state:  0.007525673687201473\n",
            "action:  0.0\n",
            "state:  0.0037310422071848304\n",
            "action:  0.0\n",
            "state:  5.970137193783235e-05\n",
            "action:  0.0\n",
            "state:  -0.004310135628423221\n",
            "action:  0.0\n",
            "state:  -0.001871409335650305\n",
            "action:  0.0\n",
            "state:  -0.002335883628259506\n",
            "action:  0.0\n",
            "state:  0.001936613709020266\n",
            "action:  0.0\n",
            "state:  0.0001267137384604404\n",
            "action:  0.0\n",
            "state:  0.0029498825992987157\n",
            "action:  0.0\n",
            "state:  -0.0034497089092379407\n",
            "action:  0.0\n",
            "state:  -0.00720330722626482\n",
            "action:  0.0\n",
            "state:  -0.0016003848613129984\n",
            "action:  0.0\n",
            "state:  0.001425090617915552\n",
            "action:  0.0\n",
            "state:  0.009412589363341488\n",
            "action:  0.0\n",
            "state:  0.0008286072265496772\n",
            "action:  0.0\n",
            "state:  0.0026387626667763618\n",
            "action:  0.0\n",
            "state:  0.004519316687791081\n",
            "action:  0.0\n",
            "state:  9.73893621877425e-05\n",
            "action:  0.0\n",
            "state:  0.0038689734131781483\n",
            "action:  0.2\n",
            "state:  0.002456759646384908\n",
            "action:  0.0\n",
            "state:  0.001094414514188486\n",
            "action:  0.0\n",
            "state:  -0.0002076058957797862\n",
            "action:  0.0\n",
            "state:  -0.0013107690322087136\n",
            "action:  0.0\n",
            "state:  -0.001340120847055578\n",
            "action:  0.0\n",
            "state:  -0.0023473197142025496\n",
            "action:  0.0\n",
            "state:  0.0006913633320925117\n",
            "action:  0.0\n",
            "state:  0.003018140809961992\n",
            "action:  0.0\n",
            "state:  0.0008206505309369905\n",
            "action:  0.0\n",
            "state:  0.003051325342322472\n",
            "action:  0.0\n",
            "state:  0.0015101726036576353\n",
            "action:  0.0\n",
            "state:  0.0008928932458982505\n",
            "action:  0.0\n",
            "state:  0.001726101304245578\n",
            "action:  0.0\n",
            "state:  0.002070854221233108\n",
            "action:  0.0\n",
            "state:  0.0066056821916167085\n",
            "action:  0.0\n",
            "state:  0.004153103372200217\n",
            "action:  0.0\n",
            "state:  0.0018413250989357422\n",
            "action:  0.0\n",
            "state:  0.0033698642764713755\n",
            "action:  0.0\n",
            "state:  0.000641422404619827\n",
            "action:  0.0\n",
            "state:  -0.004341935857558891\n",
            "action:  0.0\n",
            "state:  0.0008748338168690172\n",
            "action:  0.0\n",
            "state:  0.0009000304647625658\n",
            "action:  0.0\n",
            "state:  -0.003628900400384709\n",
            "action:  0.0\n",
            "state:  0.0011266494539786348\n",
            "action:  0.0\n",
            "state:  -0.016989338878244564\n",
            "action:  0.0\n",
            "state:  0.002607762884756028\n",
            "action:  0.0\n",
            "state:  -0.009138651606971138\n",
            "action:  0.0\n",
            "state:  -0.006578079549426104\n",
            "action:  0.0\n",
            "state:  -0.0008832843991276614\n",
            "action:  0.0\n",
            "state:  0.00800443314789029\n",
            "action:  0.0\n",
            "state:  0.005140939744550815\n",
            "action:  0.0\n",
            "state:  0.007525673687201473\n",
            "action:  0.0\n",
            "state:  0.0037310422071848304\n",
            "action:  0.0\n",
            "state:  5.970137193783235e-05\n",
            "action:  0.0\n",
            "state:  -0.004310135628423221\n",
            "action:  0.0\n",
            "state:  -0.001871409335650305\n",
            "action:  0.0\n",
            "state:  -0.002335883628259506\n",
            "action:  0.0\n",
            "state:  0.001936613709020266\n",
            "action:  0.0\n",
            "state:  0.0001267137384604404\n",
            "action:  0.0\n",
            "state:  0.0029498825992987157\n",
            "action:  0.0\n",
            "state:  -0.0034497089092379407\n",
            "action:  0.0\n",
            "state:  -0.00720330722626482\n",
            "action:  0.0\n",
            "state:  -0.0016003848613129984\n",
            "action:  0.0\n",
            "state:  0.001425090617915552\n",
            "action:  0.0\n",
            "state:  0.009412589363341488\n",
            "action:  0.0\n",
            "state:  0.0008286072265496772\n",
            "action:  0.0\n",
            "state:  0.0026387626667763618\n",
            "action:  0.0\n",
            "state:  0.004519316687791081\n",
            "action:  0.0\n",
            "state:  9.73893621877425e-05\n",
            "action:  0.0\n",
            "state:  0.0038689734131781483\n",
            "action:  0.2\n",
            "state:  0.002456759646384908\n",
            "action:  0.0\n",
            "state:  0.001094414514188486\n",
            "action:  0.0\n",
            "state:  -0.0002076058957797862\n",
            "action:  0.0\n",
            "state:  -0.0013107690322087136\n",
            "action:  0.0\n",
            "state:  -0.001340120847055578\n",
            "action:  0.0\n",
            "state:  -0.0023473197142025496\n",
            "action:  0.0\n",
            "state:  0.0006913633320925117\n",
            "action:  0.0\n",
            "state:  0.003018140809961992\n",
            "action:  0.0\n",
            "state:  0.0008206505309369905\n",
            "action:  0.0\n",
            "state:  0.003051325342322472\n",
            "action:  0.0\n",
            "state:  0.0015101726036576353\n",
            "action:  0.0\n",
            "state:  0.0008928932458982505\n",
            "action:  0.0\n",
            "state:  0.001726101304245578\n",
            "action:  0.0\n",
            "state:  0.002070854221233108\n",
            "action:  0.0\n",
            "state:  0.0066056821916167085\n",
            "action:  0.0\n",
            "state:  0.004153103372200217\n",
            "action:  0.0\n",
            "state:  0.0018413250989357422\n",
            "action:  0.0\n",
            "state:  0.0033698642764713755\n",
            "action:  0.0\n",
            "state:  0.000641422404619827\n",
            "action:  0.0\n",
            "state:  -0.004341935857558891\n",
            "action:  0.0\n",
            "state:  0.0008748338168690172\n",
            "action:  0.0\n",
            "state:  0.0009000304647625658\n",
            "action:  0.0\n",
            "state:  -0.003628900400384709\n",
            "action:  0.0\n",
            "state:  0.0011266494539786348\n",
            "action:  0.0\n",
            "state:  -0.016989338878244564\n",
            "action:  0.0\n",
            "state:  0.002607762884756028\n",
            "action:  0.0\n",
            "state:  -0.009138651606971138\n",
            "action:  0.0\n",
            "state:  -0.006578079549426104\n",
            "action:  0.0\n",
            "state:  -0.0008832843991276614\n",
            "action:  0.0\n",
            "state:  0.00800443314789029\n",
            "action:  0.0\n",
            "state:  0.005140939744550815\n",
            "action:  0.0\n",
            "state:  0.007525673687201473\n",
            "action:  0.0\n",
            "state:  0.0037310422071848304\n",
            "action:  0.0\n",
            "state:  5.970137193783235e-05\n",
            "action:  0.0\n",
            "state:  -0.004310135628423221\n",
            "action:  0.0\n",
            "state:  -0.001871409335650305\n",
            "action:  0.0\n",
            "state:  -0.002335883628259506\n",
            "action:  0.0\n",
            "state:  0.001936613709020266\n",
            "action:  0.0\n",
            "state:  0.0001267137384604404\n",
            "action:  0.0\n",
            "state:  0.0029498825992987157\n",
            "action:  0.0\n",
            "state:  -0.0034497089092379407\n",
            "action:  0.0\n",
            "state:  -0.00720330722626482\n",
            "action:  0.0\n",
            "state:  -0.0016003848613129984\n",
            "action:  0.0\n",
            "state:  0.001425090617915552\n",
            "action:  0.0\n",
            "state:  0.009412589363341488\n",
            "action:  0.0\n",
            "state:  0.0008286072265496772\n",
            "action:  0.0\n",
            "state:  0.0026387626667763618\n",
            "action:  0.0\n",
            "state:  0.004519316687791081\n",
            "action:  0.0\n",
            "state:  9.73893621877425e-05\n",
            "action:  0.0\n",
            "state:  0.0038689734131781483\n",
            "action:  0.2\n",
            "state:  0.002456759646384908\n",
            "action:  0.0\n",
            "state:  0.001094414514188486\n",
            "action:  0.0\n",
            "state:  -0.0002076058957797862\n",
            "action:  0.0\n",
            "state:  -0.0013107690322087136\n",
            "action:  0.0\n",
            "state:  -0.001340120847055578\n",
            "action:  0.0\n",
            "state:  -0.0023473197142025496\n",
            "action:  0.0\n",
            "state:  0.0006913633320925117\n",
            "action:  0.0\n",
            "state:  0.003018140809961992\n",
            "action:  0.0\n",
            "state:  0.0008206505309369905\n",
            "action:  0.0\n",
            "state:  0.003051325342322472\n",
            "action:  0.0\n",
            "state:  0.0015101726036576353\n",
            "action:  0.0\n",
            "state:  0.0008928932458982505\n",
            "action:  0.0\n",
            "state:  0.001726101304245578\n",
            "action:  0.0\n",
            "state:  0.002070854221233108\n",
            "action:  0.0\n",
            "state:  0.0066056821916167085\n",
            "action:  0.0\n",
            "state:  0.004153103372200217\n",
            "action:  0.0\n",
            "state:  0.0018413250989357422\n",
            "action:  0.0\n",
            "state:  0.0033698642764713755\n",
            "action:  0.0\n",
            "state:  0.000641422404619827\n",
            "action:  0.0\n",
            "state:  -0.004341935857558891\n",
            "action:  0.0\n",
            "state:  0.0008748338168690172\n",
            "action:  0.0\n",
            "state:  0.0009000304647625658\n",
            "action:  0.0\n",
            "state:  -0.003628900400384709\n",
            "action:  0.0\n",
            "state:  0.0011266494539786348\n",
            "action:  0.0\n",
            "state:  -0.016989338878244564\n",
            "action:  0.0\n",
            "state:  0.002607762884756028\n",
            "action:  0.0\n",
            "state:  -0.009138651606971138\n",
            "action:  0.0\n",
            "state:  -0.006578079549426104\n",
            "action:  0.0\n",
            "state:  -0.0008832843991276614\n",
            "action:  0.0\n",
            "state:  0.00800443314789029\n",
            "action:  0.0\n",
            "state:  0.005140939744550815\n",
            "action:  0.0\n",
            "state:  0.007525673687201473\n",
            "action:  0.0\n",
            "state:  0.0037310422071848304\n",
            "action:  0.0\n",
            "state:  5.970137193783235e-05\n",
            "action:  0.0\n",
            "state:  -0.004310135628423221\n",
            "action:  0.0\n",
            "state:  -0.001871409335650305\n",
            "action:  0.0\n",
            "state:  -0.002335883628259506\n",
            "action:  0.0\n",
            "state:  0.001936613709020266\n",
            "action:  0.0\n",
            "state:  0.0001267137384604404\n",
            "action:  0.0\n",
            "state:  0.0029498825992987157\n",
            "action:  0.0\n",
            "state:  -0.0034497089092379407\n",
            "action:  0.0\n",
            "state:  -0.00720330722626482\n",
            "action:  0.0\n",
            "state:  -0.0016003848613129984\n",
            "action:  0.0\n",
            "state:  0.001425090617915552\n",
            "action:  0.0\n",
            "state:  0.009412589363341488\n",
            "action:  0.0\n",
            "state:  0.0008286072265496772\n",
            "action:  0.0\n",
            "state:  0.0026387626667763618\n",
            "action:  0.0\n",
            "state:  0.004519316687791081\n",
            "action:  0.0\n",
            "state:  9.73893621877425e-05\n",
            "action:  0.0\n",
            "state:  0.0038689734131781483\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-UZfFlus5ly",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1720add3-dd48-49bf-e32e-bf557ba46143"
      },
      "source": [
        "len(episodeLogs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AtEvFsEtC08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edb57664-c4a8-42a1-8782-1ebd8a9e8254"
      },
      "source": [
        "len(episodeLogs[0].rewards)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-Xcpb0gs7qG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "453f61dd-0769-457e-d323-c112eadef538"
      },
      "source": [
        "episodeLogs[0].plot()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2mGS_u11xnt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}